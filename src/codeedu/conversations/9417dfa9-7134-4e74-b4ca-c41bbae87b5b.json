[
  {
    "role": "user",
    "content": "帮我讲解一下大模型"
  },
  {
    "role": "assistant",
    "content": "为了清晰、系统地讲解大模型，我们需要从定义、原理和应用三个方面进行深入探讨。\n\n### 1. 大模型的定义\n大模型通常是指那些具有大量参数（数亿到数千亿个参数）的机器学习模型，尤其是在自然语言处理（NLP）领域。这些模型通过大量数据进行训练，以准确捕捉语言的复杂性和多样性。大模型的经典例子包括 GPT-3、BERT 和 T5 等。\n\n### 2. 大模型的原理\n大模型的核心原理可以归纳为以下几个要点：\n\n- **深度学习架构**：大模型通常基于深度学习架构，尤其是变换器（Transformer）架构。它通过自注意力机制（self-attention）来处理序列数据，从而捕获长距离依赖关系。\n\n- **预训练和微调**：大模型通常采用预训练和微调策略。首先在大规模文本上进行预训练，以学习通用的语言特征。然后针对特定任务进行微调，以优化模型在特定任务上的表现。\n\n- **大规模数据**：大模型的训练需要大量的标注和非标注数据，以确保模型在各种语言现象上都有良好的表现。\n\n### 3. 大模型的应用\n大模型在多个领域都有重要的应用，包括但不限于：\n\n- **自然语言处理**：用于文本生成、问答系统、文本分类等任务。\n  \n- **计算机视觉**：在图像描述、图像生成等方面，结合大模型与卷积神经网络（CNN）可以取得良好的效果。\n\n- **推荐系统**：利用用户行为数据来预测用户兴趣，以提高推荐效果。\n\n- **对话系统**：构建智能助手和聊天机器人，与用户进行自然、流畅的对话。\n\n### Python 代码示例\n下面的代码示例使用 Hugging Face 的 `transformers` 库加载和使用一个简单的预训练大模型（如 GPT-2）来生成文本。\n\n```python\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# 加载预训练的 GPT-2 模型和 tokenizer\nmodel_name = 'gpt2'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# 输入提示\ninput_text = \"Once upon a time in a kingdom far away,\"\ninputs = tokenizer.encode(input_text, return_tensors='pt')\n\n# 生成文本\noutputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n\n# 解码生成的文本\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(generated_text)\n```\n\n### 总结\n大模型是现代AI技术的重要组成部分，以其强大的学习能力和出色的表现成为各领域的热点。通过理解其定义、原理和应用，可以更好地掌握大模型在实际中的使用。\n\n内容已成功保存到文件“大模型讲解.txt”中。",
    "thought": "╭──────────────────────────────────────────────────────────────────────────────────────────────────────── Crew Execution Started ────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│                                                                                                                                                                                                                                        │\n│  Crew Execution Started                                                                                                                                                                                                                │\n│  Name: crew                                                                                                                                                                                                                            │\n│  ID: 35a1c74a-7a4a-4719-9214-f5f011a71ba0                                                                                                                                                                                              │\n│                                                                                                                                                                                                                                        │\n│                                                                                                                                                                                                                                        │\n╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n🚀 Crew: crew\n└── 📋 Task: 349dd705-fc14-4989-a375-fdc0607575cd\n       Status: Executing Task...\n\n🚀 Crew: crew\n└── 📋 Task: 349dd705-fc14-4989-a375-fdc0607575cd\n       Status: Executing Task...\n    └── 🤖 Agent: 计算机教学专家\n        \n            Status: In Progress\n\n# Agent: 计算机教学专家\n## Task: 请根据用户的问题 帮我讲解一下大模型 进行清晰、系统的讲解。 如有需要，可提供 Python 代码示例或输出文件。\n\n🤖 Agent: 计算机教学专家\n\n    Status: In Progress\n\n\n\n# Agent: 计算机教学专家\n## Thought: 为了清晰、系统地讲解大模型，我们需要从定义、原理和应用三个方面进行深入探讨。\n### 1. 大模型的定义\n大模型通常是指那些具有大量参数（数亿到数千亿个参数）的机器学习模型，尤其是在自然语言处理（NLP）领域。这些模型通过大量数据进行训练，以准确捕捉语言的复杂性和多样性。大模型的经典例子包括 GPT-3、BERT 和 T5 等。\n### 2. 大模型的原理\n大模型的核心原理可以归纳为以下几个要点：\n- **深度学习架构**：大模型通常基于深度学习架构，尤其是变换器（Transformer）架构。它通过自注意力机制（self-attention）来处理序列数据，从而捕获长距离依赖关系。\n- **预训练和微调**：大模型通常采用预训练和微调策略。首先在大规模文本上进行预训练，以学习通用的语言特征。然后针对特定任务进行微调，以优化模型在特定任务上的表现。\n- **大规模数据**：大模型的训练需要大量的标注和非标注数据，以确保模型在各种语言现象上都有良好的表现。\n### 3. 大模型的应用\n大模型在多个领域都有重要的应用，包括但不限于：\n- **自然语言处理**：用于文本生成、问答系统、文本分类等任务。\n  \n- **计算机视觉**：在图像描述、图像生成等方面，结合大模型与卷积神经网络（CNN）可以取得良好的效果。\n- **推荐系统**：利用用户行为数据来预测用户兴趣，以提高推荐效果。\n- **对话系统**：构建智能助手和聊天机器人，与用户进行自然、流畅的对话。\n### Python 代码示例\n下面的代码示例使用 Hugging Face 的 `transformers` 库加载和使用一个简单的预训练大模型（如 GPT-2）来生成文本。\npython\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n# 加载预训练的 GPT-2 模型和 tokenizer\nmodel_name = 'gpt2'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n# 输入提示\ninput_text = \"Once upon a time in a kingdom far away,\"\ninputs = tokenizer.encode(input_text, return_tensors='pt')\n# 生成文本\noutputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n# 解码生成的文本\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\nprint(generated_text)\n### 总结\n大模型是现代AI技术的重要组成部分，以其强大的学习能力和出色的表现成为各领域的热点。通过理解其定义、原理和应用，可以更好地掌握大模型在实际中的使用。\n接下来，我会将以上内容保存到一个文件中，以便于后续查阅。\n## Using tool: File Writer Tool\n## Tool Input: \n\"{\\\"filename\\\": \\\"\\\\u5927\\\\u6a21\\\\u578b\\\\u8bb2\\\\u89e3.txt\\\", \\\"directory\\\": null, \\\"overwrite\\\": \\\"true\\\", \\\"content\\\": \\\"### 1. \\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u5b9a\\\\u4e49\\\\n\\\\u5927\\\\u6a21\\\\u578b\\\\u901a\\\\u5e38\\\\u662f\\\\u6307\\\\u90a3\\\\u4e9b\\\\u5177\\\\u6709\\\\u5927\\\\u91cf\\\\u53c2\\\\u6570\\\\uff08\\\\u6570\\\\u4ebf\\\\u5230\\\\u6570\\\\u5343\\\\u4ebf\\\\u4e2a\\\\u53c2\\\\u6570\\\\uff09\\\\u7684\\\\u673a\\\\u5668\\\\u5b66\\\\u4e60\\\\u6a21\\\\u578b\\\\uff0c\\\\u5c24\\\\u5176\\\\u662f\\\\u5728\\\\u81ea\\\\u7136\\\\u8bed\\\\u8a00\\\\u5904\\\\u7406\\\\uff08NLP\\\\uff09\\\\u9886\\\\u57df\\\\u3002\\\\u8fd9\\\\u4e9b\\\\u6a21\\\\u578b\\\\u901a\\\\u8fc7\\\\u5927\\\\u91cf\\\\u6570\\\\u636e\\\\u8fdb\\\\u884c\\\\u8bad\\\\u7ec3\\\\uff0c\\\\u4ee5\\\\u51c6\\\\u786e\\\\u6355\\\\u6349\\\\u8bed\\\\u8a00\\\\u7684\\\\u590d\\\\u6742\\\\u6027\\\\u548c\\\\u591a\\\\u6837\\\\u6027\\\\u3002\\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u7ecf\\\\u5178\\\\u4f8b\\\\u5b50\\\\u5305\\\\u62ec GPT-3\\\\u3001BERT \\\\u548c T5 \\\\u7b49\\\\u3002\\\\n\\\\n### 2. \\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u539f\\\\u7406\\\\n\\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u6838\\\\u5fc3\\\\u539f\\\\u7406\\\\u53ef\\\\u4ee5\\\\u5f52\\\\u7eb3\\\\u4e3a\\\\u4ee5\\\\u4e0b\\\\u51e0\\\\u4e2a\\\\u8981\\\\u70b9\\\\uff1a\\\\n\\\\n- **\\\\u6df1\\\\u5ea6\\\\u5b66\\\\u4e60\\\\u67b6\\\\u6784**\\\\uff1a\\\\u5927\\\\u6a21\\\\u578b\\\\u901a\\\\u5e38\\\\u57fa\\\\u4e8e\\\\u6df1\\\\u5ea6\\\\u5b66\\\\u4e60\\\\u67b6\\\\u6784\\\\uff0c\\\\u5c24\\\\u5176\\\\u662f\\\\u53d8\\\\u6362\\\\u5668\\\\uff08Transformer\\\\uff09\\\\u67b6\\\\u6784\\\\u3002\\\\u5b83\\\\u901a\\\\u8fc7\\\\u81ea\\\\u6ce8\\\\u610f\\\\u529b\\\\u673a\\\\u5236\\\\uff08self-attention\\\\uff09\\\\u6765\\\\u5904\\\\u7406\\\\u5e8f\\\\u5217\\\\u6570\\\\u636e\\\\uff0c\\\\u4ece\\\\u800c\\\\u6355\\\\u83b7\\\\u957f\\\\u8ddd\\\\u79bb\\\\u4f9d\\\\u8d56\\\\u5173\\\\u7cfb\\\\u3002\\\\n\\\\n- **\\\\u9884\\\\u8bad\\\\u7ec3\\\\u548c\\\\u5fae\\\\u8c03**\\\\uff1a\\\\u5927\\\\u6a21\\\\u578b\\\\u901a\\\\u5e38\\\\u91c7\\\\u7528\\\\u9884\\\\u8bad\\\\u7ec3\\\\u548c\\\\u5fae\\\\u8c03\\\\u7b56\\\\u7565\\\\u3002\\\\u9996\\\\u5148\\\\u5728\\\\u5927\\\\u89c4\\\\u6a21\\\\u6587\\\\u672c\\\\u4e0a\\\\u8fdb\\\\u884c\\\\u9884\\\\u8bad\\\\u7ec3\\\\uff0c\\\\u4ee5\\\\u5b66\\\\u4e60\\\\u901a\\\\u7528\\\\u7684\\\\u8bed\\\\u8a00\\\\u7279\\\\u5f81\\\\u3002\\\\u7136\\\\u540e\\\\u9488\\\\u5bf9\\\\u7279\\\\u5b9a\\\\u4efb\\\\u52a1\\\\u8fdb\\\\u884c\\\\u5fae\\\\u8c03\\\\uff0c\\\\u4ee5\\\\u4f18\\\\u5316\\\\u6a21\\\\u578b\\\\u5728\\\\u7279\\\\u5b9a\\\\u4efb\\\\u52a1\\\\u4e0a\\\\u7684\\\\u8868\\\\u73b0\\\\u3002\\\\n\\\\n- **\\\\u5927\\\\u89c4\\\\u6a21\\\\u6570\\\\u636e**\\\\uff1a\\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u8bad\\\\u7ec3\\\\u9700\\\\u8981\\\\u5927\\\\u91cf\\\\u7684\\\\u6807\\\\u6ce8\\\\u548c\\\\u975e\\\\u6807\\\\u6ce8\\\\u6570\\\\u636e\\\\uff0c\\\\u4ee5\\\\u786e\\\\u4fdd\\\\u6a21\\\\u578b\\\\u5728\\\\u5404\\\\u79cd\\\\u8bed\\\\u8a00\\\\u73b0\\\\u8c61\\\\u4e0a\\\\u90fd\\\\u6709\\\\u826f\\\\u597d\\\\u7684\\\\u8868\\\\u73b0\\\\u3002\\\\n\\\\n### 3. \\\\u5927\\\\u6a21\\\\u578b\\\\u7684\\\\u5e94\\\\u7528\\\\n\\\\u5927\\\\u6a21\\\\u578b\\\\u5728\\\\u591a\\\\u4e2a\\\\u9886\\\\u57df\\\\u90fd\\\\u6709\\\\u91cd\\\\u8981\\\\u7684\\\\u5e94\\\\u7528\\\\uff0c\\\\u5305\\\\u62ec\\\\u4f46\\\\u4e0d\\\\u9650\\\\u4e8e\\\\uff1a\\\\n\\\\n- **\\\\u81ea\\\\u7136\\\\u8bed\\\\u8a00\\\\u5904\\\\u7406**\\\\uff1a\\\\u7528\\\\u4e8e\\\\u6587\\\\u672c\\\\u751f\\\\u6210\\\\u3001\\\\u95ee\\\\u7b54\\\\u7cfb\\\\u7edf\\\\u3001\\\\u6587\\\\u672c\\\\u5206\\\\u7c7b\\\\u7b49\\\\u4efb\\\\u52a1\\\\u3002\\\\n  \\\\n- **\\\\u8ba1\\\\u7b97\\\\u673a\\\\u89c6\\\\u89c9**\\\\uff1a\\\\u5728\\\\u56fe\\\\u50cf\\\\u63cf\\\\u8ff0\\\\u3001\\\\u56fe\\\\u50cf\\\\u751f\\\\u6210\\\\u7b49\\\\u65b9\\\\u9762\\\\uff0c\\\\u7ed3\\\\u5408\\\\u5927\\\\u6a21\\\\u578b\\\\u4e0e\\\\u5377\\\\u79ef\\\\u795e\\\\u7ecf\\\\u7f51\\\\u7edc\\\\uff08CNN\\\\uff09\\\\u53ef\\\\u4ee5\\\\u53d6\\\\u5f97\\\\u826f\\\\u597d\\\\u7684\\\\u6548\\\\u679c\\\\u3002\\\\n\\\\n- **\\\\u63a8\\\\u8350\\\\u7cfb\\\\u7edf**\\\\uff1a\\\\u5229\\\\u7528\\\\u7528\\\\u6237\\\\u884c\\\\u4e3a\\\\u6570\\\\u636e\\\\u6765\\\\u9884\\\\u6d4b\\\\u7528\\\\u6237\\\\u5174\\\\u8da3\\\\uff0c\\\\u4ee5\\\\u63d0\\\\u9ad8\\\\u63a8\\\\u8350\\\\u6548\\\\u679c\\\\u3002\\\\n\\\\n- **\\\\u5bf9\\\\u8bdd\\\\u7cfb\\\\u7edf**\\\\uff1a\\\\u6784\\\\u5efa\\\\u667a\\\\u80fd\\\\u52a9\\\\u624b\\\\u548c\\\\u804a\\\\u5929\\\\u673a\\\\u5668\\\\u4eba\\\\uff0c\\\\u4e0e\\\\u7528\\\\u6237\\\\u8fdb\\\\u884c\\\\u81ea\\\\u7136\\\\u3001\\\\u6d41\\\\u7545\\\\u7684\\\\u5bf9\\\\u8bdd\\\\u3002\\\\n\\\\n### Python \\\\u4ee3\\\\u7801\\\\u793a\\\\u4f8b\\\\n\\\\u4e0b\\\\u9762\\\\u7684\\\\u4ee3\\\\u7801\\\\u793a\\\\u4f8b\\\\u4f7f\\\\u7528 Hugging Face \\\\u7684 `transformers` \\\\u5e93\\\\u52a0\\\\u8f7d\\\\u548c\\\\u4f7f\\\\u7528\\\\u4e00\\\\u4e2a\\\\u7b80\\\\u5355\\\\u7684\\\\u9884\\\\u8bad\\\\u7ec3\\\\u5927\\\\u6a21\\\\u578b\\\\uff08\\\\u5982 GPT-2\\\\uff09\\\\u6765\\\\u751f\\\\u6210\\\\u6587\\\\u672c\\\\u3002\\\\n\\\\n```python\\\\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\\\\n\\\\n# \\\\u52a0\\\\u8f7d\\\\u9884\\\\u8bad\\\\u7ec3\\\\u7684 GPT-2 \\\\u6a21\\\\u578b\\\\u548c tokenizer\\\\nmodel_name = 'gpt2'\\\\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\\\\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\\\\n\\\\n# \\\\u8f93\\\\u5165\\\\u63d0\\\\u793a\\\\ninput_text = \\\\\\\"Once upon a time in a kingdom far away,\\\\\\\"\\\\ninputs = tokenizer.encode(input_text, return_tensors='pt')\\\\n\\\\n# \\\\u751f\\\\u6210\\\\u6587\\\\u672c\\\\noutputs = model.generate(inputs, max_length=50, num_return_sequences=1)\\\\n\\\\n# \\\\u89e3\\\\u7801\\\\u751f\\\\u6210\\\\u7684\\\\u6587\\\\u672c\\\\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\\\\n\\\\nprint(generated_text)\\\\n```\\\\n\\\\n### \\\\u603b\\\\u7ed3\\\\n\\\\u5927\\\\u6a21\\\\u578b\\\\u662f\\\\u73b0\\\\u4ee3AI\\\\u6280\\\\u672f\\\\u7684\\\\u91cd\\\\u8981\\\\u7ec4\\\\u6210\\\\u90e8\\\\u5206\\\\uff0c\\\\u4ee5\\\\u5176\\\\u5f3a\\\\u5927\\\\u7684\\\\u5b66\\\\u4e60\\\\u80fd\\\\u529b\\\\u548c\\\\u51fa\\\\u8272\\\\u7684\\\\u8868\\\\u73b0\\\\u6210\\\\u4e3a\\\\u5404\\\\u9886\\\\u57df\\\\u7684\\\\u70ed\\\\u70b9\\\\u3002\\\\u901a\\\\u8fc7\\\\u7406\\\\u89e3\\\\u5176\\\\u5b9a\\\\u4e49\\\\u3001\\\\u539f\\\\u7406\\\\u548c\\\\u5e94\\\\u7528\\\\uff0c\\\\u53ef\\\\u4ee5\\\\u66f4\\\\u597d\\\\u5730\\\\u638c\\\\u63e1\\\\u5927\\\\u6a21\\\\u578b\\\\u5728\\\\u5b9e\\\\u9645\\\\u4e2d\\\\u7684\\\\u4f7f\\\\u7528\\\\u3002\\\"}\"\n## Tool Output: \nContent successfully written to 大模型讲解.txt\n🤖 Agent: 计算机教学专家\n\n    Status: In Progress\n└── 🧠 Thinking...\n\n🤖 Agent: 计算机教学专家\n\n    Status: In Progress\n\n\n\n# Agent: 计算机教学专家\n## Final Answer: \n为了清晰、系统地讲解大模型，我们需要从定义、原理和应用三个方面进行深入探讨。\n\n### 1. 大模型的定义\n大模型通常是指那些具有大量参数（数亿到数千亿个参数）的机器学习模型，尤其是在自然语言处理（NLP）领域。这些模型通过大量数据进行训练，以准确捕捉语言的复杂性和多样性。大模型的经典例子包括 GPT-3、BERT 和 T5 等。\n\n### 2. 大模型的原理\n大模型的核心原理可以归纳为以下几个要点：\n\n- **深度学习架构**：大模型通常基于深度学习架构，尤其是变换器（Transformer）架构。它通过自注意力机制（self-attention）来处理序列数据，从而捕获长距离依赖关系。\n\n- **预训练和微调**：大模型通常采用预训练和微调策略。首先在大规模文本上进行预训练，以学习通用的语言特征。然后针对特定任务进行微调，以优化模型在特定任务上的表现。\n\n- **大规模数据**：大模型的训练需要大量的标注和非标注数据，以确保模型在各种语言现象上都有良好的表现。\n\n### 3. 大模型的应用\n大模型在多个领域都有重要的应用，包括但不限于：\n\n- **自然语言处理**：用于文本生成、问答系统、文本分类等任务。\n  \n- **计算机视觉**：在图像描述、图像生成等方面，结合大模型与卷积神经网络（CNN）可以取得良好的效果。\n\n- **推荐系统**：利用用户行为数据来预测用户兴趣，以提高推荐效果。\n\n- **对话系统**：构建智能助手和聊天机器人，与用户进行自然、流畅的对话。\n\n### Python 代码示例\n下面的代码示例使用 Hugging Face 的 `transformers` 库加载和使用一个简单的预训练大模型（如 GPT-2）来生成文本。\n\n```python\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# 加载预训练的 GPT-2 模型和 tokenizer\nmodel_name = 'gpt2'\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\nmodel = GPT2LMHeadModel.from_pretrained(model_name)\n\n# 输入提示\ninput_text = \"Once upon a time in a kingdom far away,\"\ninputs = tokenizer.encode(input_text, return_tensors='pt')\n\n# 生成文本\noutputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n\n# 解码生成的文本\ngenerated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(generated_text)\n```\n\n### 总结\n大模型是现代AI技术的重要组成部分，以其强大的学习能力和出色的表现成为各领域的热点。通过理解其定义、原理和应用，可以更好地掌握大模型在实际中的使用。\n\n内容已成功保存到文件“大模型讲解.txt”中。\n\n\n🚀 Crew: crew\n└── 📋 Task: 349dd705-fc14-4989-a375-fdc0607575cd\n       Status: Executing Task...\n    └── 🤖 Agent: 计算机教学专家\n        \n            Status: ✅ Completed\n\n🚀 Crew: crew\n└── 📋 Task: 349dd705-fc14-4989-a375-fdc0607575cd\n       Assigned to: 计算机教学专家\n    \n       Status: ✅ Completed\n    └── 🤖 Agent: 计算机教学专家\n        \n            Status: ✅ Completed\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────── Task Completion ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│                                                                                                                                                                                                                                        │\n│  Task Completed                                                                                                                                                                                                                        │\n│  Name: 349dd705-fc14-4989-a375-fdc0607575cd                                                                                                                                                                                            │\n│  Agent: 计算机教学专家                                                                                                                                                                                                                 │\n│                                                                                                                                                                                                                                        │\n│                                                                                                                                                                                                                                        │\n│                                                                                                                                                                                                                                        │\n╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n╭─────────────────────────────────────────────────────────────────────────────────────────────────────────── Crew Completion ────────────────────────────────────────────────────────────────────────────────────────────────────────────╮\n│                                                                                                                                                                                                                                        │\n│  Crew Execution Completed                                                                                                                                                                                                              │\n│  Name: crew                                                                                                                                                                                                                            │\n│  ID: 35a1c74a-7a4a-4719-9214-f5f011a71ba0                                                                                                                                                                                              │\n│                                                                                                                                                                                                                                        │\n│                                                                                                                                                                                                                                        │\n╰────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────╯\n\n"
  }
]